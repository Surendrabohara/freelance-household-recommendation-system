{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da8c60c-dbf8-47c6-95a8-e972ea862c34",
   "metadata": {},
   "source": [
    "# Data processing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e2008092-3c44-4579-9494-f2feaa2377b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f110b50d-00b6-4439-9de9-d03c7f7dd358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the data from the CSV file\n",
    "df = pd.read_csv('householddata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24174508-a54c-4a34-98bf-5a3777821983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert the date columns to datetime objects\n",
    "df['Start Time'] = pd.to_datetime(df['Start Time'])\n",
    "df['End Time'] = pd.to_datetime(df['End Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67184381-84cc-45e7-afab-ecabbfbab686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "954412f1-5f9f-4bd1-aa44-6abb94eabf41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df.drop(['Title', 'Description', 'Location', 'Review'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa16382e-2f34-4a78-9751-9987e46ddce8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the categorical variables to numerical variables\n",
    "df['Status'] = pd.Categorical(df['Status'], categories=['requested', 'in-progress', 'completed', 'rejected'], ordered=True)\n",
    "df['Status'] = df['Status'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d28b9ea2-ceae-418b-9496-01dda40b5ed3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the usernames to numerical IDs\n",
    "df['Customer ID'] = df['Customer Username'].astype('category').cat.codes\n",
    "df['Worker ID'] = df['Worker Username'].astype('category').cat.codes\n",
    "df.drop(['Customer Username', 'Worker Username'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fca58581-144a-4b6a-86bf-5df397215860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Start Time            End Time  Status  Rating  Hourly Rate  \\\n",
      "5  2023-10-30 20:00:30 2023-10-30 22:00:30       2     4.0    40.707462   \n",
      "13 2023-08-06 14:21:21 2023-08-06 16:21:21       2     0.0    31.252497   \n",
      "14 2023-12-07 19:14:42 2023-12-07 21:14:42       2     1.0    44.312673   \n",
      "21 2023-01-16 23:52:14 2023-01-17 01:52:14       2     4.0    33.625232   \n",
      "22 2023-08-26 17:29:17 2023-08-26 19:29:17       2     4.0    22.309144   \n",
      "\n",
      "    Total Cost  Customer ID  Worker ID  \n",
      "5    81.414924            5          2  \n",
      "13   62.504993            5          4  \n",
      "14   88.625345            5          0  \n",
      "21   67.250465            5          3  \n",
      "22   44.618289            5          1  \n"
     ]
    }
   ],
   "source": [
    "# Display the processed data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11f2eca-fe8f-46e6-88ff-02c28565611d",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7e4ec14-984f-4286-8f8c-43fbecf45a92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Shuffle the rows of the data\n",
    "shuffled_data = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b29a6027-dd4d-4915-9293-279f740e11fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Split the data into training and testing sets\n",
    "num_rows = shuffled_data.shape[0]\n",
    "train_size = int(0.8 * num_rows)\n",
    "train_data = shuffled_data.iloc[:train_size]\n",
    "test_data = shuffled_data.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0224a2d3-3aea-4b9e-a0e8-91e6a33f0dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to calculate the Pearson correlation coefficient\n",
    "def pearson_correlation(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    n = len(x)\n",
    "    xy = np.multiply(x, y)\n",
    "    sum_x = sum(x)\n",
    "    sum_y = sum(y)\n",
    "    sum_xy = sum(xy)\n",
    "    sum_x_squared = sum(x ** 2)\n",
    "    sum_y_squared = sum(y ** 2)\n",
    "    numerator = n * sum_xy - sum_x * sum_y\n",
    "    denominator = np.sqrt((n * sum_x_squared - sum_x ** 2) * (n * sum_y_squared - sum_y ** 2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7dd2920-05ef-44b7-925c-1021114d0fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to predict the rating for a worker based on a customer's ratings and the Pearson correlation coefficients\n",
    "def predict_rating(worker_ratings, correlation_coefficients, customer_ratings):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(len(worker_ratings)):\n",
    "        if worker_ratings[i] != 0 and customer_ratings[i] != 0:\n",
    "            numerator += correlation_coefficients[i] * (customer_ratings[i] - np.mean(customer_ratings))\n",
    "            denominator += np.abs(correlation_coefficients[i])\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(worker_ratings) + (numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4cf6cf9f-87c6-4317-bd0f-9d7e919b7e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to calculate the mean absolute error (MAE) between the predicted ratings and the actual ratings\n",
    "def calculate_mae(predictions, actual_ratings):\n",
    "    n = len(predictions)\n",
    "    error = sum([abs(predictions[i] - actual_ratings[i]) for i in range(n)])\n",
    "    return error / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c16b3e8-af44-411f-976b-1505b3271338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation_coefficients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m y \u001b[38;5;241m=\u001b[39m common_ratings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating_work\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     13\u001b[0m correlation_coefficient \u001b[38;5;241m=\u001b[39m pearson_correlation(x, y)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mcorrelation_coefficients\u001b[49m[i][j] \u001b[38;5;241m=\u001b[39m correlation_coefficient\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correlation_coefficients' is not defined"
     ]
    }
   ],
   "source": [
    "#Calculate the Pearson correlation coefficients between each pair of customers and workers\n",
    "num_customers = len(df['Customer ID'].unique())\n",
    "num_workers = len(df['Worker ID'].unique())\n",
    "correlation_matrix = np.zeros((num_customers, num_workers))\n",
    "for i in range(num_customers):\n",
    "    customer_ratings = train_data[train_data['Customer ID'] == i]['Rating'].tolist()\n",
    "    for j in range(num_workers):\n",
    "        worker_ratings = train_data[train_data['Worker ID'] == j]['Rating'].tolist()\n",
    "        common_ratings = (train_data[train_data['Customer ID'] == i]\n",
    "                          .merge(train_data[train_data['Worker ID'] == j], on='Start Time', suffixes=('_cust', '_work')))\n",
    "        x = common_ratings['Rating_cust'].tolist()\n",
    "        y = common_ratings['Rating_work'].tolist()\n",
    "        correlation_coefficient = pearson_correlation(x, y)\n",
    "        correlation_coefficients[i][j] = correlation_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e77a4-f534-419e-8af7-7b8d27e89f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00d2c605-9a30-4caf-a388-287728cbbc36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "num_rows = shuffled_data.shape[0]\n",
    "train_size = int(0.8 * num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7b67fa6-c7b9-4d0d-b57b-acbe2fc1efd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = shuffled_data.iloc[:train_size]\n",
    "test_data = shuffled_data.iloc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "926676a7-d5e9-4ed4-a973-6889a04c73b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a dictionary to store the user ratings from the training data\n",
    "user_ratings = {}\n",
    "for i in range(train_data.shape[0]):\n",
    "    customer_id = train_data.loc[i, 'Customer ID']\n",
    "    worker_id = train_data.loc[i, 'Worker ID']\n",
    "    rating = train_data.loc[i, 'Rating']\n",
    "    if customer_id not in user_ratings:\n",
    "        user_ratings[customer_id] = {}\n",
    "    user_ratings[customer_id][worker_id] = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bea282ae-fd70-489a-ad5b-fb44a5806a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to calculate the Pearson correlation coefficient between two users\n",
    "from math import sqrt\n",
    "\n",
    "def pearson_correlation(user1_ratings, user2_ratings):\n",
    "    shared_items = set(user1_ratings.keys()) & set(user2_ratings.keys())\n",
    "    num_items = len(shared_items)\n",
    "    if num_items == 0:\n",
    "        return 0\n",
    "    sum1 = sum([user1_ratings[item] for item in shared_items])\n",
    "    sum2 = sum([user2_ratings[item] for item in shared_items])\n",
    "    sum1_sq = sum([pow(user1_ratings[item], 2) for item in shared_items])\n",
    "    sum2_sq = sum([pow(user2_ratings[item], 2) for item in shared_items])\n",
    "    product_sum = sum([user1_ratings[item] * user2_ratings[item] for item in shared_items])\n",
    "    numerator = product_sum - (sum1 * sum2 / num_items)\n",
    "    denominator = sqrt((sum1_sq - pow(sum1, 2) / num_items) * (sum2_sq - pow(sum2, 2) / num_items))\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "409dcbf0-816e-4d21-a90a-585262af440b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a function to predict the rating for a given user and item based on the training data\n",
    "def predict_rating(user_ratings, user, item):\n",
    "    similarities = []\n",
    "    for other_user in user_ratings:\n",
    "        if other_user != user and item in user_ratings[other_user]:\n",
    "            similarity = pearson_correlation(user_ratings[user], user_ratings[other_user])\n",
    "            if similarity > 0:\n",
    "                similarities.append((other_user, similarity))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    if len(similarities) == 0:\n",
    "        return None\n",
    "    numerator = sum([user_ratings[sim_user][item] * sim for sim_user, sim in similarities])\n",
    "    denominator = sum([sim for sim_user, sim in similarities])\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "31158dbd-94c0-4246-9b82-8fbcf97655cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loop through the testing data and use the predict_rating function to predict the ratings\n",
    "predicted_ratings = []\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "for i in range(test_data.shape[0]):\n",
    "    customer_id = test_data.loc[i, 'Customer ID']\n",
    "    worker_id = test_data.loc[i, 'Worker ID']\n",
    "    actual_rating = test_data.loc[i, 'Rating']\n",
    "    predicted_rating = predict_rating(user_ratings, customer_id, worker_id)\n",
    "    if predicted_rating is not None:\n",
    "        predicted_ratings.append((actual_rating, predicted_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a2b3749-6afa-4c0e-b0e0-9c6f4981b626",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'tuple' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mabs\u001b[39m(predictions[i] \u001b[38;5;241m-\u001b[39m actual_ratings[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m error \u001b[38;5;241m/\u001b[39m n\n\u001b[1;32m----> 9\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_ratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactual_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mae)\n",
      "Cell \u001b[1;32mIn[46], line 6\u001b[0m, in \u001b[0;36mcalculate_mae\u001b[1;34m(predictions, actual_ratings)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_mae\u001b[39m(predictions, actual_ratings):\n\u001b[0;32m      5\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(predictions)\n\u001b[1;32m----> 6\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[38;5;28;43mabs\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactual_ratings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m error \u001b[38;5;241m/\u001b[39m n\n",
      "Cell \u001b[1;32mIn[46], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_mae\u001b[39m(predictions, actual_ratings):\n\u001b[0;32m      5\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(predictions)\n\u001b[1;32m----> 6\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mabs\u001b[39m(\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactual_ratings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n)])\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m error \u001b[38;5;241m/\u001b[39m n\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'tuple' and 'float'"
     ]
    }
   ],
   "source": [
    "actual_ratings = list(test_data['Rating'])\n",
    "\n",
    "#Calculate the accuracy of the predictions using the mean absolute error (MAE)\n",
    "def calculate_mae(predictions, actual_ratings):\n",
    "    n = len(predictions)\n",
    "    error = sum([abs(predictions[i] - actual_ratings[i]) for i in range(n)])\n",
    "    return error / n\n",
    "\n",
    "mae = calculate_mae(predicted_ratings, actual_ratings)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e091d1f-439f-4642-9491-7e234f1f169a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ab0ee-7ee3-416e-8a16-813d00c046f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba090f3e-34a6-41bf-8f52-ed95a6600e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e81a9011-1221-4cfa-b6bd-0aaa48b6edcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Axis must be specified when shapes of a and weights differ.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 132\u001b[0m\n\u001b[0;32m    130\u001b[0m         predicted_rating \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(ratings)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m         predicted_rating \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     predicted_rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:532\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m wgt\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    533\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAxis must be specified when shapes of a and weights \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wgt\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    537\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1D weights expected when shapes of a and weights differ.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Axis must be specified when shapes of a and weights differ."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from the CSV file\n",
    "df = pd.read_csv('householddata.csv')\n",
    "\n",
    "# Convert the date columns to datetime objects\n",
    "df['Start Time'] = pd.to_datetime(df['Start Time'])\n",
    "df['End Time'] = pd.to_datetime(df['End Time'])\n",
    "\n",
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(['Title', 'Description', 'Location', 'Review'], axis=1, inplace=True)\n",
    "\n",
    "# Convert the categorical variables to numerical variables\n",
    "df['Status'] = pd.Categorical(df['Status'], categories=['requested', 'in-progress', 'completed', 'rejected'], ordered=True)\n",
    "df['Status'] = df['Status'].cat.codes\n",
    "\n",
    "# Convert the usernames to numerical IDs\n",
    "df['Customer ID'] = df['Customer Username'].astype('category').cat.codes\n",
    "df['Worker ID'] = df['Worker Username'].astype('category').cat.codes\n",
    "df.drop(['Customer Username', 'Worker Username'], axis=1, inplace=True)\n",
    "\n",
    "# Shuffle the rows of the data\n",
    "shuffled_data = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "num_rows = shuffled_data.shape[0]\n",
    "train_size = int(0.8 * num_rows)\n",
    "train_data = shuffled_data.iloc[:train_size]\n",
    "test_data = shuffled_data.iloc[train_size:]\n",
    "\n",
    "# Define a function to calculate the Pearson correlation coefficient\n",
    "def pearson_correlation(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    n = len(x)\n",
    "    xy = x * y\n",
    "    sum_x = sum(x)\n",
    "    sum_y = sum(y)\n",
    "    sum_xy = sum(xy)\n",
    "    sum_x_sq = sum(x ** 2)\n",
    "    sum_y_sq = sum(y ** 2)\n",
    "    numerator = (n * sum_xy) - (sum_x * sum_y)\n",
    "    denominator = np.sqrt((n * sum_x_sq - sum_x**2) * (n * sum_y_sq - sum_y**2))\n",
    "    if denominator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return numerator / denominator\n",
    "\n",
    "# Define a function to predict the rating for a worker based on a customer's ratings and the Pearson correlation coefficients\n",
    "def predict_rating(worker_ratings, correlation_coefficients, customer_ratings):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(len(worker_ratings)):\n",
    "        if worker_ratings[i] != 0 and customer_ratings[i] != 0:\n",
    "            numerator += correlation_coefficients[i] * (customer_ratings[i] - np.mean(customer_ratings))\n",
    "            denominator += np.abs(correlation_coefficients[i])\n",
    "    if denominator == 0:\n",
    "        return np.mean(worker_ratings)\n",
    "    elif np.sum(correlation_coefficients) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        # Calculate the weights\n",
    "        weights = [correlation_coefficients[i] for i in range(len(worker_ratings)) if worker_ratings[i] != 0 and customer_ratings[i] != 0]\n",
    "        # Calculate the ratings\n",
    "        ratings = [worker_ratings[i] for i in range(len(worker_ratings)) if worker_ratings[i] != 0 and customer_ratings[i] != 0]\n",
    "        # Check if the weights are all zeros\n",
    "        if np.sum(weights) == 0:\n",
    "            # If all weights are zero, use unweighted mean\n",
    "            predicted_rating = np.mean(ratings)\n",
    "        else:\n",
    "            # If some weights are non-zero, use weighted mean\n",
    "            predicted_rating = np.average(ratings, weights=weights)\n",
    "        return np.mean(worker_ratings) + (numerator / denominator)\n",
    "\n",
    "\n",
    "# Define a function to calculate the mean absolute error (MAE) between the predicted ratings and the actual ratings\n",
    "def calculate_mae(predictions, actual_ratings):\n",
    "    n = len(predictions)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "    error = sum([abs(predictions[i] - actual_ratings[i]) for i in range(n)])\n",
    "    return error / n\n",
    "\n",
    "\n",
    "# Calculate the Pearson correlation coefficients between each pair of customers and workers\n",
    "num_customers = len(df['Customer ID'].unique())\n",
    "num_workers = len(df['Worker ID'].unique())\n",
    "correlation_matrix = np.zeros((num_customers, num_workers))\n",
    "for i in range(num_customers):\n",
    "    for j in range(num_workers):\n",
    "        customer_ratings = []\n",
    "        worker_ratings = []\n",
    "        for index, row in train_data.iterrows():\n",
    "            if row['Customer ID'] == i and row['Worker ID'] == j:\n",
    "                customer_ratings.append(row['Rating'])\n",
    "                if row['Rating'] != 0:\n",
    "                    worker_ratings.append(row['Rating'])\n",
    "        if len(customer_ratings) == len(worker_ratings):\n",
    "            correlation_matrix[i, j] = pearson_correlation(customer_ratings, worker_ratings)\n",
    "\n",
    "\n",
    "# Make predictions on the test set using the correlation matrix\n",
    "predicted_ratings = []\n",
    "actual_ratings = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    customer_id = test_data.iloc[i]['Customer ID']\n",
    "    worker_id = test_data.iloc[i]['Worker ID']\n",
    "    actual_rating = test_data.iloc[i]['Rating']\n",
    "    # Find the workers with the highest correlations for this customer\n",
    "    worker_correlations = correlation_matrix[customer_id]\n",
    "    top_worker_ids = np.argsort(worker_correlations)[::-1][:5]\n",
    "\n",
    "# Calculate the predicted rating as the weighted average of the top worker ratings\n",
    "weights = np.array([worker_correlations[id] for id in top_worker_ids])\n",
    "ratings = []\n",
    "for id in top_worker_ids:\n",
    "    worker_ratings = []\n",
    "    for index, row in train_data.iterrows():\n",
    "        if row['Customer ID'] == customer_id and row['Worker ID'] == id:\n",
    "            worker_ratings.append(row['Rating'])\n",
    "    if len(worker_ratings) > 0:\n",
    "        ratings.append(sum(worker_ratings) / len(worker_ratings))\n",
    "ratings = np.array(ratings)\n",
    "if len(ratings) > 0:\n",
    "    if np.sum(weights) == 0:\n",
    "        predicted_rating = np.mean(ratings)\n",
    "    else:\n",
    "        predicted_rating = np.average(ratings, weights=weights)\n",
    "else:\n",
    "    predicted_rating = 0.0\n",
    "\n",
    "\n",
    "# Calculate the accuracy of the predictions using the mean absolute error (MAE)\n",
    "mae = calculate_mae(predicted_ratings, actual_ratings)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "#Save the correlation matrix to a file\n",
    "np.savetxt('correlation_matrix.csv', correlation_matrix, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552cba5-ea7b-4b43-b9a8-8626ec96eea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf96fb6-2230-492a-a49f-51b4f1354333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5b60d-e43c-48b1-844d-5b4f763469f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25a480ca-e821-40f7-b12d-505d130549b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Customer ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhouseholddata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Pivot the dataset to get the ratings matrix\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m ratings_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCustomer ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWorker ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate the correlation matrix\u001b[39;00m\n\u001b[0;32m     11\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m ratings_matrix\u001b[38;5;241m.\u001b[39mcorr(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m'\u001b[39m, min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8731\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   8714\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8715\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   8716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8727\u001b[0m     sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   8728\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   8729\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[1;32m-> 8731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8732\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8733\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8736\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:97\u001b[0m, in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m     94\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:166\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(values)\n\u001b[1;32m--> 166\u001b[0m grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table dropped a column because it failed to aggregate. This behavior \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis deprecated and will raise in a future version of pandas. Select only the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns that can be aggregated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m )\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rewrite_warning(\n\u001b[0;32m    173\u001b[0m     target_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default value of numeric_only\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    174\u001b[0m     target_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m     new_message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[0;32m    176\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8410\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Customer ID'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('householddata.csv')\n",
    "\n",
    "# Pivot the dataset to get the ratings matrix\n",
    "ratings_matrix = data.pivot_table(index='Customer ID', columns='Worker ID', values='Rating')\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = ratings_matrix.corr(method='pearson', min_periods=5)\n",
    "r = sum((x - mean(x)) * (y - mean(y))) / (sqrt(sum((x - mean(x)) ** 2)) * sqrt(sum((y - mean(y)) ** 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932853fa-545d-4a32-a02b-625d29f3cc8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757327a-05f9-4d83-9f20-19f95f853975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d7df141e-6206-4784-bcfa-2aeedd7ce5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'jessica33'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'jessica33'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m actual_rating \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Find the workers with the highest correlations for this customer\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m worker_correlations \u001b[38;5;241m=\u001b[39m \u001b[43mcorrelation_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcustomer_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m top_worker_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(worker_correlations)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m5\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate the predicted rating as the weighted average of the top worker ratings\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'jessica33'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('householddata.csv')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data.sample(frac=0.7, random_state=1)\n",
    "test_data = data.drop(train_data.index)\n",
    "\n",
    "# Calculate the Pearson correlation coefficients between customers and workers in the training set\n",
    "correlation_matrix = train_data.pivot_table(index='Customer Username', columns='Worker Username', values='Rating').corr()\n",
    "\n",
    "# Make predictions on the test set using the correlation matrix\n",
    "predicted_ratings = []\n",
    "actual_ratings = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    customer_id = test_data.iloc[i]['Customer Username']\n",
    "    worker_id = test_data.iloc[i]['Worker Username']\n",
    "    actual_rating = test_data.iloc[i]['Rating']\n",
    "    # Find the workers with the highest correlations for this customer\n",
    "    worker_correlations = correlation_matrix[customer_id]\n",
    "    top_worker_ids = np.argsort(worker_correlations)[::-1][:5]\n",
    "\n",
    "    # Calculate the predicted rating as the weighted average of the top worker ratings\n",
    "    weights = [worker_correlations[id] for id in top_worker_ids]\n",
    "    ratings = []\n",
    "    for id in top_worker_ids:\n",
    "        worker_ratings = []\n",
    "        for index, row in train_data.iterrows():\n",
    "            if row['Customer Username'] == customer_id and row['Worker Username'] == id:\n",
    "                worker_ratings.append(row['Rating'])\n",
    "        if len(worker_ratings) > 0:\n",
    "            ratings.append(sum(worker_ratings) / len(worker_ratings))\n",
    "    if len(ratings) > 0:\n",
    "        if np.sum(weights) == 0:\n",
    "            predicted_rating = np.mean(ratings)\n",
    "        else:\n",
    "            predicted_rating = np.average(ratings, weights=weights)\n",
    "    else:\n",
    "        predicted_rating = 0.0\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "    actual_ratings.append(actual_rating)\n",
    "\n",
    "# Calculate the accuracy of the predictions using the mean absolute error (MAE)\n",
    "mae = np.mean(np.abs(np.array(predicted_ratings) - np.array(actual_ratings)))\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07425d-5b05-418d-ac2f-24da65d3f18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa732bdc-0d8c-4a56-8c4a-8d03dbfcbea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c261c0e1-6cb7-406b-ad3f-18b34b15c95e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sulav\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared score: -0.21572768883569737\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('householddata.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "data = data.dropna()\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "# Split the data\n",
    "X = data.drop(['Rating'], axis=1)\n",
    "y = data['Rating']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate the Pearson correlation coefficient\n",
    "correlations = []\n",
    "for feature in X_train.columns:\n",
    "    correlation, _ = pearsonr(X_train[feature], y_train)\n",
    "    correlations.append((feature, correlation))\n",
    "correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Select the features\n",
    "num_features = 5\n",
    "selected_features = [correlation[0] for correlation in correlations[:num_features]]\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.score(X_test[selected_features], y_test)\n",
    "print(f'R-squared score: {score}')\n",
    "\n",
    "# Make predictions\n",
    "new_data = pd.read_csv('householddata.csv')\n",
    "new_data = pd.get_dummies(new_data)\n",
    "predictions = model.predict(new_data[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c0dc1d9d-f386-4fa4-9bd1-07ce48d38a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pearson_corr_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpearson_correlation_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m----> 5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mpearson_corr_model\u001b[49m, file)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pearson_corr_model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "with open('pearson_correlation_model.pkl', 'wb') as file:\n",
    "    pickle.dump(pearson_corr_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f70dd-33a4-4492-9f88-3b5ae24ed28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8ae4e199-0adb-47c4-b070-f17e1e9743f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pickle\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"householddata.csv\")\n",
    "\n",
    "# Pivot the data to create a matrix of worker ratings\n",
    "ratings_df = df.pivot_table(index=\"Worker Username\", columns=\"Customer Username\", values=\"Rating\")\n",
    "\n",
    "# Replace missing ratings with 0\n",
    "ratings_df.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the Pearson Correlation Coefficient\n",
    "pearson_corr = 1 - pairwise_distances(ratings_df, metric=\"correlation\")\n",
    "\n",
    "# Save the Pearson Correlation model to a pickle file\n",
    "with open(\"pearson_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pearson_corr, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84825d-6201-4b79-a84f-9ef189f8fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ff794-8e01-40dd-9aaa-5647a7eaca7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "06c47920-859b-42d0-8691-2af417323738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from math import sqrt\n",
    "\n",
    "# Load the data from the CSV file\n",
    "data = []\n",
    "with open(\"householddata.csv\", newline=\"\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    header = next(reader)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Compute the average rating for each worker\n",
    "worker_ratings = {}\n",
    "for row in data:\n",
    "    worker = row[1]\n",
    "    rating = int(row[8]) if row[8] != '' else 0\n",
    "    if worker not in worker_ratings:\n",
    "        worker_ratings[worker] = []\n",
    "    worker_ratings[worker].append(rating)\n",
    "worker_averages = {}\n",
    "for worker, ratings in worker_ratings.items():\n",
    "    worker_averages[worker] = sum(ratings) / len(ratings)\n",
    "\n",
    "# Compute the Pearson correlation coefficient between each pair of workers\n",
    "correlations = {}\n",
    "for i in range(len(worker_averages)):\n",
    "    for j in range(i + 1, len(worker_averages)):\n",
    "        worker1 = list(worker_averages.keys())[i]\n",
    "        worker2 = list(worker_averages.keys())[j]\n",
    "        ratings1 = worker_ratings[worker1]\n",
    "        ratings2 = worker_ratings[worker2]\n",
    "        avg1 = worker_averages[worker1]\n",
    "        avg2 = worker_averages[worker2]\n",
    "        numerator = 0\n",
    "        denom1 = 0\n",
    "        denom2 = 0\n",
    "        for k in range(len(ratings1)):\n",
    "            numerator += (ratings1[k] - avg1) * (ratings2[k] - avg2)\n",
    "            denom1 += (ratings1[k] - avg1) ** 2\n",
    "            denom2 += (ratings2[k] - avg2) ** 2\n",
    "        denominator = sqrt(denom1) * sqrt(denom2)\n",
    "        if denominator != 0:\n",
    "            correlation = numerator / denominator\n",
    "            correlations[(worker1, worker2)] = correlation\n",
    "\n",
    "# Save the correlations to a file\n",
    "with open(\"worker_correlations.csv\", \"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Worker 1\", \"Worker 2\", \"Correlation\"])\n",
    "    for (worker1, worker2), correlation in correlations.items():\n",
    "        writer.writerow([worker1, worker2, correlation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5de0ef-ea43-4b27-ad80-bf43ba374d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
